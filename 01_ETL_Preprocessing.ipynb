{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c356c651",
   "metadata": {},
   "source": [
    "# WESM Price Prediction - ETL/Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f480b3",
   "metadata": {},
   "source": [
    "## Setup and \"Loader\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934ee86",
   "metadata": {},
   "source": [
    "This part sets up the method for loading and concatenating of *.csv* files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d6d0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "PATH_GWAP = \"./raw_data/GWAP/\"\n",
    "PATH_RTD = \"./raw_data/RTD_Regional/\"\n",
    "PATH_OUTAGES = \"./raw_data/Outages/\"\n",
    "\n",
    "# Connects extracted CSV files\n",
    "def load_and_concatenate_csvs(path):\n",
    "    files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    print(f\"File Count in {path}: {len(files)}\")\n",
    "\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        dfs.append(df)\n",
    "\n",
    "    concatenated_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    print(f\"Concatenated DataFrame Shape from {path}: {concatenated_df.shape}\")\n",
    "    \n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f9700",
   "metadata": {},
   "source": [
    "## GWAP Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb75231",
   "metadata": {},
   "source": [
    "For this portion, we load all GWAP (Generation Weighted Average Price) CSV files into a single DataFrame, `df_gwap`. For the purposes of this project, we retain only the rows where `REGION_NAME` is `CLUZ`, restricting the dataset to Luzon GWAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb72ab96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Count in ./raw_data/GWAP/: 54\n",
      "Concatenated DataFrame Shape from ./raw_data/GWAP/: (77814, 6)\n",
      "Final GWAP DataFrame Shape: (15552, 2)\n",
      "               datetime       GWAP\n",
      "167 2025-11-02 00:05:00  2321.6376\n",
      "172 2025-11-02 00:10:00  2394.5301\n",
      "112 2025-11-02 00:15:00  2325.4293\n",
      "117 2025-11-02 00:20:00  2320.9318\n",
      "37  2025-11-02 00:25:00  2314.6513\n"
     ]
    }
   ],
   "source": [
    "df_gwap = load_and_concatenate_csvs(PATH_GWAP)\n",
    "df_gwap = df_gwap[df_gwap[\"REGION_NAME\"] == \"CLUZ\"].copy()\n",
    "df_gwap[\"datetime\"] = pd.to_datetime(df_gwap[\"TIME_INTERVAL\"], format='mixed', dayfirst=False)\n",
    "df_gwap = df_gwap[[\"datetime\", \"GWAP\"]].sort_values(\"datetime\")\n",
    "df_gwap = df_gwap.drop_duplicates(subset=['datetime']) # just in case\n",
    "\n",
    "print(f\"Final GWAP DataFrame Shape: {df_gwap.shape}\")\n",
    "print(df_gwap.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09354c04",
   "metadata": {},
   "source": [
    "## RTD Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebcf8f7",
   "metadata": {},
   "source": [
    "This portion loads all RTD Regional (Real-Time Dispatch Regional) CSV files, retaining only rows where `REGION_NAME` is `CLUZ`.\n",
    "\n",
    "Next, rows where `COMMODITY_TYPE` is `En` are extracted and stored in `df_energy`, as this commodity represents actual energy in the RTD market. The retained columns are:\n",
    "- `datetime`\n",
    "- `MKT_REQT`, renamed to `energy_demand_mw`\n",
    "- `GENERATION`, renamed to `energy_supply_mw`\n",
    "- Additionally, an engineered feature, `energy_shortage_mw`, is added. This represents the difference between energy demand and supply and serves as an indicator of potential load shedding or brownouts.\n",
    "\n",
    "All remaining rows where `COMMODITY_TYPE != En` are treated as reserve commodities and stored in `df_reserves`. These rows are grouped by `datetime`, with the following aggregated columns retained:\n",
    "- `datetime`\n",
    "- `MKT_REQT`, summed and renamed to `reserve_demand_mw`\n",
    "- `GENERATION`, summed and renamed to `reserve_supply_mw`\n",
    "\n",
    "As a final step, `df_energy` and `df_reserves` are merged on `datetime` to produce `df_X`, which contains the final set of RTD-derived features used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b0aee3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Count in ./raw_data/RTD_Regional/: 54\n",
      "Concatenated DataFrame Shape from ./raw_data/RTD_Regional/: (233274, 13)\n",
      "Final RTD Features DataFrame Shape: (15548, 6)\n",
      "             datetime  energy_demand_mw  energy_supply_mw  energy_shortage_mw  \\\n",
      "0 2025-11-02 00:05:00           8347.43           8524.83             -177.40   \n",
      "1 2025-11-02 00:10:00           8332.95           8537.22             -204.27   \n",
      "2 2025-11-02 00:15:00           8293.18           8504.03             -210.85   \n",
      "3 2025-11-02 00:20:00           8249.05           8440.88             -191.83   \n",
      "4 2025-11-02 00:25:00           8243.93           8414.21             -170.28   \n",
      "\n",
      "   reserve_demand_mw  reserve_supply_mw  \n",
      "0             1431.0             1431.0  \n",
      "1             1431.0             1431.0  \n",
      "2             1431.0             1431.0  \n",
      "3             1431.0             1431.0  \n",
      "4             1431.0             1431.0  \n"
     ]
    }
   ],
   "source": [
    "df_rtd = load_and_concatenate_csvs(PATH_RTD)\n",
    "df_rtd = df_rtd[df_rtd[\"REGION_NAME\"] == \"CLUZ\"].copy()\n",
    "df_rtd[\"datetime\"] = pd.to_datetime(df_rtd[\"TIME_INTERVAL\"], format='mixed', dayfirst=False)\n",
    "\n",
    "# Extract Energy (Demand & Supply)\n",
    "# Commodity Type \"En\" refers to energy\n",
    "df_energy = df_rtd[df_rtd[\"COMMODITY_TYPE\"] == \"En\"].copy()\n",
    "df_energy = df_energy[[\"datetime\", \"MKT_REQT\", \"GENERATION\"]]\n",
    "df_energy.columns = [\"datetime\", \"energy_demand_mw\", \"energy_supply_mw\"]\n",
    "\n",
    "# Engineered Feature: Energy Shortage (Demand - Supply)\n",
    "# If > 0, load shedding / brownouts likely\n",
    "df_energy['energy_shortage_mw'] = df_energy['energy_demand_mw'] - df_energy['energy_supply_mw']\n",
    "\n",
    "# Extract Reserves (Safety Net)\n",
    "# Anything that is not \"En\" are reserves\n",
    "df_reserves = df_rtd[df_rtd[\"COMMODITY_TYPE\"] != \"En\"].copy()\n",
    "\n",
    "# Group reserves by datetime then sum them up\n",
    "df_reserves = df_reserves.groupby(\"datetime\")[[\"MKT_REQT\", \"GENERATION\"]].sum().reset_index()\n",
    "df_reserves.columns = [\"datetime\", \"reserve_demand_mw\", \"reserve_supply_mw\"]\n",
    "\n",
    "# Note: There used to be a reserve_shortage column but after EDA, it was found to be mostly zeros (apart from one point) so it was removed.\n",
    "\n",
    "# Merge them back\n",
    "df_X = pd.merge(df_energy, df_reserves, on=\"datetime\", how=\"left\")\n",
    "df_X = df_X.fillna(0)  # Fill NaNs with 0\n",
    "\n",
    "print(f\"Final RTD Features DataFrame Shape: {df_X.shape}\")\n",
    "print(df_X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e73e8",
   "metadata": {},
   "source": [
    "## Outage Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5704b488",
   "metadata": {},
   "source": [
    "This portion loads all Outages CSV files into a single DataFrame, `df_outages`.\n",
    "\n",
    "First, rows with missing `RESOURCE_NAME` are dropped, as these entries cannot be reliably associated with a specific generator.\n",
    "\n",
    "Next, the dataset is filtered to include only Luzon plants, identified by the first two digits of `RESOURCE_NAME` (`01`, `02`, `03`). A new column, `prefix`, is created for this purpose, and only rows with Luzon prefixes are retained.\n",
    "\n",
    "A `datetime` column is then created from the `RUN_TIME` column to enable proper time-series alignment.\n",
    "\n",
    "Finally, the outages are aggregated by `datetime`, resulting in a new DataFrame, `df_out_count`, which contains the total count of Luzon outages per time interval. This provides a concise, time-aligned representation of outages for further analysis and integration with RTD and GWAP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba7cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Count in ./raw_data/Outages/: 54\n",
      "Concatenated DataFrame Shape from ./raw_data/Outages/: (235619, 7)\n",
      "Dropped 54 rows with missing Resource Name.\n",
      "Filtered for Luzon (Prefixes 01-03). Remaining rows: 77484\n",
      "Final Outages DataFrame Shape: (15548, 2)\n",
      "             datetime  outage_count\n",
      "0 2025-11-02 00:05:00             4\n",
      "1 2025-11-02 00:10:00             4\n",
      "2 2025-11-02 00:15:00             4\n",
      "3 2025-11-02 00:20:00             4\n",
      "4 2025-11-02 00:25:00             4\n"
     ]
    }
   ],
   "source": [
    "df_outages = load_and_concatenate_csvs(PATH_OUTAGES)\n",
    "\n",
    "# Drop rows with missing Resource Name\n",
    "initial_len = len(df_outages)\n",
    "df_outages = df_outages.dropna(subset=['RESOURCE_NAME'])\n",
    "print(f\"Dropped {initial_len - len(df_outages)} rows with missing Resource Name.\")\n",
    "\n",
    "# Filter to Luzon plants only (Prefix 01-03)\n",
    "df_outages['prefix'] = df_outages['RESOURCE_NAME'].astype(str).str[:2]\n",
    "luzon_prefixes = ['01', '02', '03']\n",
    "df_outages = df_outages[df_outages['prefix'].isin(luzon_prefixes)].copy()\n",
    "print(f\"Filtered for Luzon (Prefixes 01-03). Remaining rows: {len(df_outages)}\")\n",
    "\n",
    "df_outages['datetime'] = pd.to_datetime(df_outages['RUN_TIME'], format='mixed', dayfirst=False)\n",
    "df_out_count = df_outages.groupby('datetime').size().reset_index(name='outage_count') # count of outages per datetime\n",
    "\n",
    "print(f\"Final Outages DataFrame Shape: {df_out_count.shape}\")\n",
    "print(df_out_count.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902a3ef",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafcf9f0",
   "metadata": {},
   "source": [
    "This portion merges the previously processed GWAP, RTD, and Outages datasets to create a single, time-aligned dataset for analysis.\n",
    "\n",
    "First, `df_gwap` (Luzon GWAP prices) is merged with `df_X` (RTD features including energy demand/supply, reserves, and energy shortage) on `datetime` using an inner join. This ensures that only timestamps present in both datasets are retained.\n",
    "\n",
    "Next, the outage data (`df_out_count`) is merged using a left join, adding the `outage_count` feature to the dataset. Missing values are filled with 0, indicating that no outages occurred at those timestamps.\n",
    "\n",
    "To capture temporal dependencies in electricity prices, lag features are created from GWAP:\n",
    "- `GWAP_Lag_1` → GWAP value 5 minutes ago  \n",
    "- `GWAP_Lag_12` → GWAP value 1 hour ago  \n",
    "- `GWAP_Lag_288` → GWAP value at the same time yesterday (24 hours ago)  \n",
    "\n",
    "Rows with missing values, resulting from the lag feature creation, are dropped.\n",
    "\n",
    "The final dataset is saved to `final_dataset.csv` and contains the following features for each timestamp:\n",
    "- GWAP\n",
    "- Energy demand, supply, and shortage  \n",
    "- Reserve demand and supply  \n",
    "- Outage count  \n",
    "- Lagged GWAP values\n",
    "\n",
    "This dataset is now fully prepared for exploratory data analysis, visualization, or predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f29bf548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Shape: (15260, 11)\n",
      "Date Range: 2025-11-03 00:05:00 to 2025-12-26 00:00:00\n",
      "               datetime       GWAP  energy_demand_mw  energy_supply_mw  \\\n",
      "288 2025-11-03 00:05:00  2590.3569           8549.80           8592.66   \n",
      "289 2025-11-03 00:10:00  2379.0282           8502.47           8621.92   \n",
      "290 2025-11-03 00:15:00  2378.7328           8503.43           8621.24   \n",
      "291 2025-11-03 00:20:00  2378.4848           8446.04           8586.57   \n",
      "292 2025-11-03 00:25:00  2403.6429           8474.76           8589.58   \n",
      "\n",
      "     energy_shortage_mw  reserve_demand_mw  reserve_supply_mw  outage_count  \\\n",
      "288              -42.86             1368.0             1368.0           3.0   \n",
      "289             -119.45             1368.0             1368.0           3.0   \n",
      "290             -117.81             1368.0             1368.0           3.0   \n",
      "291             -140.53             1368.0             1368.0           3.0   \n",
      "292             -114.82             1368.0             1368.0           3.0   \n",
      "\n",
      "     GWAP_Lag_1  GWAP_Lag_12  GWAP_Lag_288  \n",
      "288   3127.6873    2528.3538     2321.6376  \n",
      "289   2590.3569    2534.1721     2394.5301  \n",
      "290   2379.0282    2484.7641     2325.4293  \n",
      "291   2378.7328    2720.1748     2320.9318  \n",
      "292   2378.4848    2756.0153     2314.6513  \n"
     ]
    }
   ],
   "source": [
    "# Merge GWAP with RTD features\n",
    "final_df = pd.merge(df_gwap, df_X, on=\"datetime\", how=\"inner\")\n",
    "\n",
    "# Merge with Outages data\n",
    "final_df = pd.merge(final_df, df_out_count, on=\"datetime\", how=\"left\")\n",
    "final_df['outage_count'] = final_df['outage_count'].fillna(0)  # Fill NaNs with 0\n",
    "\n",
    "# Time Lags (for autocorrelation)\n",
    "final_df['GWAP_Lag_1'] = final_df['GWAP'].shift(1)    # 5 mins ago\n",
    "final_df['GWAP_Lag_12'] = final_df['GWAP'].shift(12)  # 1 hour ago\n",
    "final_df['GWAP_Lag_288'] = final_df['GWAP'].shift(288) # 24 hours ago (Yesterday same time)\n",
    "\n",
    "final_df = final_df.dropna()\n",
    "final_df.to_csv(\"final_dataset.csv\", index=False)\n",
    "\n",
    "print(f\"Final Dataset Shape: {final_df.shape}\")\n",
    "print(f\"Date Range: {final_df['datetime'].min()} to {final_df['datetime'].max()}\")\n",
    "print(final_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
