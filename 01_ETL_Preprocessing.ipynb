{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c356c651",
   "metadata": {},
   "source": [
    "# WESM Price Prediction - ETL/Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f480b3",
   "metadata": {},
   "source": [
    "## Setup and \"Loader\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934ee86",
   "metadata": {},
   "source": [
    "This part sets up the method for loading and concatenating of *.csv* files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6d0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "PATH_GWAP = \"./raw_data/GWAP/\"\n",
    "PATH_RTD = \"./raw_data/RTD_Regional/\"\n",
    "PATH_OUTAGES = \"./raw_data/Outages/\"\n",
    "\n",
    "# Connects extracted CSV files\n",
    "def load_and_concatenate_csvs(path):\n",
    "    files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    print(f\"File Count in {path}: {len(files)}\")\n",
    "\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        dfs.append(df)\n",
    "\n",
    "    concatenated_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    print(f\"Concatenated DataFrame Shape from {path}: {concatenated_df.shape}\")\n",
    "    \n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f9700",
   "metadata": {},
   "source": [
    "## GWAP Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb75231",
   "metadata": {},
   "source": [
    "For this portion, we load all GWAP (Generation Weighted Average Price) CSV files into a single DataFrame, `df_gwap`. For the purposes of this project, we retain only the rows where `REGION_NAME` is `CLUZ`, restricting the dataset to Luzon GWAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb72ab96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Count in ./raw_data/GWAP/: 90\n",
      "Concatenated DataFrame Shape from ./raw_data/GWAP/: (129690, 6)\n",
      "Final GWAP DataFrame Shape: (25920, 2)\n",
      "               datetime       GWAP\n",
      "366 2025-10-28 00:05:00  2800.5685\n",
      "371 2025-10-28 00:10:00  2985.7148\n",
      "314 2025-10-28 00:15:00  2899.6850\n",
      "377 2025-10-28 00:20:00  2992.3605\n",
      "416 2025-10-28 00:25:00  2949.7466\n"
     ]
    }
   ],
   "source": [
    "df_gwap = load_and_concatenate_csvs(PATH_GWAP)\n",
    "df_gwap = df_gwap[df_gwap[\"REGION_NAME\"] == \"CLUZ\"].copy()\n",
    "df_gwap[\"datetime\"] = pd.to_datetime(df_gwap[\"TIME_INTERVAL\"], format='mixed', dayfirst=False)\n",
    "df_gwap = df_gwap[[\"datetime\", \"GWAP\"]].sort_values(\"datetime\")\n",
    "df_gwap = df_gwap.drop_duplicates(subset=['datetime']) # just in case\n",
    "\n",
    "print(f\"Final GWAP DataFrame Shape: {df_gwap.shape}\")\n",
    "print(df_gwap.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09354c04",
   "metadata": {},
   "source": [
    "## RTD Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebcf8f7",
   "metadata": {},
   "source": [
    "This portion loads all RTD Regional (Real-Time Dispatch Regional) CSV files, retaining only rows where `REGION_NAME` is `CLUZ`.\n",
    "\n",
    "Next, rows where `COMMODITY_TYPE` is `En` are extracted and stored in `df_energy`, as this commodity represents actual energy in the RTD market. The retained columns are:\n",
    "- `datetime`\n",
    "- `MKT_REQT`, renamed to `energy_demand_mw`\n",
    "- `GENERATION`, renamed to `energy_supply_mw`\n",
    "- Additionally, an engineered feature, `energy_shortage_mw`, is added. This represents the difference between energy demand and supply and serves as an indicator of potential load shedding or brownouts.\n",
    "\n",
    "All remaining rows where `COMMODITY_TYPE != En` are treated as reserve commodities and stored in `df_reserves`. These rows are grouped by `datetime`, with the following aggregated columns retained:\n",
    "- `datetime`\n",
    "- `MKT_REQT`, summed and renamed to `reserve_demand_mw`\n",
    "- `GENERATION`, summed and renamed to `reserve_supply_mw`\n",
    "\n",
    "As a final step, `df_energy` and `df_reserves` are merged on `datetime` to produce `df_X`, which contains the final set of RTD-derived features used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b0aee3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Count in ./raw_data/RTD_Regional/: 90\n",
      "Concatenated DataFrame Shape from ./raw_data/RTD_Regional/: (388815, 13)\n",
      "Final RTD Features DataFrame Shape: (25915, 6)\n",
      "             datetime  energy_demand_mw  energy_supply_mw  energy_shortage_mw  \\\n",
      "0 2025-10-28 00:05:00           9043.68           9095.22              -51.54   \n",
      "1 2025-10-28 00:10:00           9002.76           9038.49              -35.73   \n",
      "2 2025-10-28 00:15:00           8975.73           9026.83              -51.10   \n",
      "3 2025-10-28 00:20:00           9027.64           9012.70               14.94   \n",
      "4 2025-10-28 00:25:00           9003.70           9001.07                2.63   \n",
      "\n",
      "   reserve_demand_mw  reserve_supply_mw  \n",
      "0             1495.0             1495.0  \n",
      "1             1495.0             1495.0  \n",
      "2             1495.0             1495.0  \n",
      "3             1495.0             1495.0  \n",
      "4             1495.0             1495.0  \n"
     ]
    }
   ],
   "source": [
    "df_rtd = load_and_concatenate_csvs(PATH_RTD)\n",
    "df_rtd = df_rtd[df_rtd[\"REGION_NAME\"] == \"CLUZ\"].copy()\n",
    "df_rtd[\"datetime\"] = pd.to_datetime(df_rtd[\"TIME_INTERVAL\"], format='mixed', dayfirst=False)\n",
    "\n",
    "# Extract Energy (Demand & Supply)\n",
    "# Commodity Type \"En\" refers to energy\n",
    "df_energy = df_rtd[df_rtd[\"COMMODITY_TYPE\"] == \"En\"].copy()\n",
    "df_energy = df_energy[[\"datetime\", \"MKT_REQT\", \"GENERATION\"]]\n",
    "df_energy.columns = [\"datetime\", \"energy_demand_mw\", \"energy_supply_mw\"]\n",
    "\n",
    "# Engineered Feature: Energy Shortage (Demand - Supply)\n",
    "# If > 0, load shedding / brownouts likely\n",
    "df_energy['energy_shortage_mw'] = df_energy['energy_demand_mw'] - df_energy['energy_supply_mw']\n",
    "\n",
    "# Extract Reserves (Safety Net)\n",
    "# Anything that is not \"En\" are reserves\n",
    "df_reserves = df_rtd[df_rtd[\"COMMODITY_TYPE\"] != \"En\"].copy()\n",
    "\n",
    "# Group reserves by datetime then sum them up\n",
    "df_reserves = df_reserves.groupby(\"datetime\")[[\"MKT_REQT\", \"GENERATION\"]].sum().reset_index()\n",
    "df_reserves.columns = [\"datetime\", \"reserve_demand_mw\", \"reserve_supply_mw\"]\n",
    "\n",
    "# Note: There used to be a reserve_shortage column but after EDA, it was found to be mostly zeros (apart from one point) so it was removed.\n",
    "\n",
    "# Merge them back\n",
    "df_X = pd.merge(df_energy, df_reserves, on=\"datetime\", how=\"left\")\n",
    "df_X = df_X.fillna(0)  # Fill NaNs with 0\n",
    "\n",
    "print(f\"Final RTD Features DataFrame Shape: {df_X.shape}\")\n",
    "print(df_X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e73e8",
   "metadata": {},
   "source": [
    "## Outage Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5704b488",
   "metadata": {},
   "source": [
    "This portion loads all Outages CSV files into a single DataFrame, `df_outages`.\n",
    "\n",
    "First, rows with missing `RESOURCE_NAME` are dropped, as these entries cannot be reliably associated with a specific generator.\n",
    "\n",
    "Next, the dataset is filtered to include only Luzon plants, identified by the first two digits of `RESOURCE_NAME` (`01`, `02`, `03`). A new column, `prefix`, is created for this purpose, and only rows with Luzon prefixes are retained.\n",
    "\n",
    "A `datetime` column is then created from the `RUN_TIME` column to enable proper time-series alignment.\n",
    "\n",
    "Finally, the outages are aggregated by `datetime`, resulting in a new DataFrame, `df_out_count`, which contains the total count of Luzon outages per time interval. This provides a concise, time-aligned representation of outages for further analysis and integration with RTD and GWAP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba7cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Count in ./raw_data/Outages/: 90\n",
      "Concatenated DataFrame Shape from ./raw_data/Outages/: (345237, 7)\n",
      "Dropped 90 rows with missing Resource Name.\n",
      "Filtered for Luzon (Prefixes 01-03). Remaining rows: 117926\n",
      "Final Outages DataFrame Shape: (25915, 2)\n",
      "             datetime  outage_count\n",
      "0 2025-10-28 00:05:00             4\n",
      "1 2025-10-28 00:10:00             4\n",
      "2 2025-10-28 00:15:00             4\n",
      "3 2025-10-28 00:20:00             4\n",
      "4 2025-10-28 00:25:00             4\n"
     ]
    }
   ],
   "source": [
    "df_outages = load_and_concatenate_csvs(PATH_OUTAGES)\n",
    "\n",
    "# Drop rows with missing Resource Name\n",
    "initial_len = len(df_outages)\n",
    "df_outages = df_outages.dropna(subset=['RESOURCE_NAME'])\n",
    "print(f\"Dropped {initial_len - len(df_outages)} rows with missing Resource Name.\")\n",
    "\n",
    "# Filter to Luzon plants only (Prefix 01-03)\n",
    "df_outages['prefix'] = df_outages['RESOURCE_NAME'].astype(str).str[:2]\n",
    "luzon_prefixes = ['01', '02', '03']\n",
    "df_outages = df_outages[df_outages['prefix'].isin(luzon_prefixes)].copy()\n",
    "print(f\"Filtered for Luzon (Prefixes 01-03). Remaining rows: {len(df_outages)}\")\n",
    "\n",
    "df_outages['datetime'] = pd.to_datetime(df_outages['RUN_TIME'], format='mixed', dayfirst=False)\n",
    "df_out_count = df_outages.groupby('datetime').size().reset_index(name='outage_count') # count of outages per datetime\n",
    "\n",
    "print(f\"Final Outages DataFrame Shape: {df_out_count.shape}\")\n",
    "print(df_out_count.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902a3ef",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafcf9f0",
   "metadata": {},
   "source": [
    "This portion merges the previously processed GWAP, RTD, and Outages datasets to create a single, time-aligned dataset for analysis.\n",
    "\n",
    "First, `df_gwap` (Luzon GWAP prices) is merged with `df_X` (RTD features including energy demand/supply, reserves, and energy shortage) on `datetime` using an inner join. This ensures that only timestamps present in both datasets are retained.\n",
    "\n",
    "Next, the outage data (`df_out_count`) is merged using a left join, adding the `outage_count` feature to the dataset. Missing values are filled with 0, indicating that no outages occurred at those timestamps.\n",
    "\n",
    "To capture temporal dependencies in electricity prices, lag features are created from GWAP:\n",
    "- `GWAP_Lag_1` → GWAP value 5 minutes ago  \n",
    "- `GWAP_Lag_12` → GWAP value 1 hour ago  \n",
    "- `GWAP_Lag_288` → GWAP value at the same time yesterday (24 hours ago)  \n",
    "\n",
    "Rows with missing values, resulting from the lag feature creation, are dropped.\n",
    "\n",
    "The final dataset is saved to `final_dataset.csv` and contains the following features for each timestamp:\n",
    "- GWAP\n",
    "- Energy demand, supply, and shortage  \n",
    "- Reserve demand and supply  \n",
    "- Outage count  \n",
    "- Lagged GWAP values\n",
    "\n",
    "This dataset is now fully prepared for exploratory data analysis, visualization, or predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29bf548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Shape: (25627, 11)\n",
      "Date Range: 2025-10-29 00:05:00 to 2026-01-26 00:00:00\n",
      "               datetime       GWAP  energy_demand_mw  energy_supply_mw  \\\n",
      "288 2025-10-29 00:05:00  2258.5866           9504.45           9705.25   \n",
      "289 2025-10-29 00:10:00  2913.4257           9483.49           9648.89   \n",
      "290 2025-10-29 00:15:00  2883.5764           9434.35           9627.98   \n",
      "291 2025-10-29 00:20:00  2888.6233           9417.62           9614.19   \n",
      "292 2025-10-29 00:25:00  2887.9553           9379.48           9575.17   \n",
      "\n",
      "     energy_shortage_mw  reserve_demand_mw  reserve_supply_mw  outage_count  \\\n",
      "288             -200.80             1428.0             1428.0           4.0   \n",
      "289             -165.40             1428.0             1428.0           4.0   \n",
      "290             -193.63             1428.0             1428.0           4.0   \n",
      "291             -196.57             1428.0             1428.0           4.0   \n",
      "292             -195.69             1428.0             1428.0           4.0   \n",
      "\n",
      "     GWAP_Lag_1  GWAP_Lag_12  GWAP_Lag_288  \n",
      "288   2911.7934    2977.5806     2800.5685  \n",
      "289   2258.5866    3134.3441     2985.7148  \n",
      "290   2913.4257    3134.7814     2899.6850  \n",
      "291   2883.5764    3028.0328     2992.3605  \n",
      "292   2888.6233    3028.4820     2949.7466  \n"
     ]
    }
   ],
   "source": [
    "# Merge GWAP with RTD features\n",
    "final_df = pd.merge(df_gwap, df_X, on=\"datetime\", how=\"inner\")\n",
    "\n",
    "# Merge with Outages data\n",
    "final_df = pd.merge(final_df, df_out_count, on=\"datetime\", how=\"left\")\n",
    "final_df['outage_count'] = final_df['outage_count'].fillna(0)  # Fill NaNs with 0\n",
    "\n",
    "# Time Lags (for autocorrelation)\n",
    "final_df['GWAP_Lag_1'] = final_df['GWAP'].shift(1)    # 5 mins ago\n",
    "final_df['GWAP_Lag_12'] = final_df['GWAP'].shift(12)  # 1 hour ago\n",
    "final_df['GWAP_Lag_288'] = final_df['GWAP'].shift(288) # 24 hours ago (Yesterday same time)\n",
    "\n",
    "final_df = final_df.dropna()\n",
    "final_df.to_csv(\"final_dataset.csv\", index=False)\n",
    "\n",
    "print(f\"Final Dataset Shape: {final_df.shape}\")\n",
    "print(f\"Date Range: {final_df['datetime'].min()} to {final_df['datetime'].max()}\")\n",
    "print(final_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
